{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aef879-e320-40fa-9086-99b49d96a10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa910a36-bbc8-4dfd-bf56-05a75c600b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:74: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:99: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:74: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:74: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:99: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:74: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\vaish\\AppData\\Local\\Temp\\ipykernel_7480\\1396032683.py:74: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  with open(f\"{file_path}\\{filename}.txt\", 'w+',encoding='utf-8') as file1: #Creating text files by there names and inserting article title and text\n",
      "C:\\Users\\vaish\\AppData\\Local\\Temp\\ipykernel_7480\\1396032683.py:99: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  with open(f\"{file_path}\\{filename}.txt\", 'w+') as file1:\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import urllib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "#nltk.data.path.append(\"C:\\\\Users\\\\vaish\\\\Desktop\\\\project\\\\\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "class data_ingestion:        \n",
    "    \n",
    "    def primary(self):\n",
    "        try:\n",
    "            data = pd.read_excel(os.path.join(\"Desktop/project\",\"Input.xlsx\"))\n",
    "            #print(data.head())\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f'Error {e}')\n",
    "     \n",
    "    def secondary(self):\n",
    "        \"\"\"Ingesting data from a given directory and scrape those link by using beautifulsoup and returns a dataframe\"\"\"\n",
    "        data = pd.read_excel('C:\\\\Users\\\\vaish\\\\Desktop\\\\project\\\\Input.xlsx')\n",
    "        df = data.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "        updated_list = []\n",
    "        No_Matching_Data = []\n",
    "        Blank_link = {}\n",
    "        \n",
    "        for i, url in enumerate(df['URL']):\n",
    "            response_code = requests.get(url) # creates an HTTP GET request to the specified URL\n",
    "            soup = bs(response_code.text, 'html.parser') #Contains HTML contents  \n",
    "            article_title = soup.find('title').text # to find the title tag\n",
    "            \n",
    "            all_text_element = soup.find(\"div\", class_=\"td-post-content tagdiv-type\") #BeautifulSoup to find a <div> element with a specific class attribute within the parsed HTML content.\n",
    "\n",
    "            if all_text_element is not None: # Conditional Statement if data is extracted by this specify div or not\n",
    "                all_text = all_text_element.get_text(strip=True, separator='\\n') # If this element contain a text then those text saved in all text variables\n",
    "                firstdata = all_text.splitlines() # In above code separator is used so now we split each lines\n",
    "            else:\n",
    "                print(f\"No matching element found in the HTML for URL: {url}\") # If no any text in this div element then this url along with url_id store in blank dictionary\n",
    "                firstdata = []        \n",
    "                Blank_link[f\"blackassign00{i+1}\"] = url        \n",
    "                Blank = {\n",
    "                        'URL_ID' : f\"blackassign00{i+1}\" ,\n",
    "                        'URL'    : url \n",
    "                        }\n",
    "                No_Matching_Data.append(Blank)\n",
    "                \n",
    "            # new dataframe store all the extracted text \n",
    "            new_dataframe = {\n",
    "                    \"URL_ID\": df[\"URL_ID\"][i],\n",
    "                    'URL' : url,\n",
    "                    'article_words':f\"{article_title}-{firstdata}\"\n",
    "                }    \n",
    "            \n",
    "            updated_list.append(new_dataframe)\n",
    "\n",
    "            filename = urllib.parse.quote_plus(url) # this will convert the url as url name for the text file\n",
    "            file_path = 'C:\\\\Users\\\\vaish\\\\Desktop\\\\project\\\\text_files' #Path to store text files\n",
    "            space = \" \"\n",
    "                \n",
    "            with open(f\"{file_path}\\{filename}.txt\", 'w+',encoding='utf-8') as file1: #Creating text files by there names and inserting article title and text\n",
    "                file1.writelines(article_title)\n",
    "                file1.writelines(space)\n",
    "                if firstdata is None:\n",
    "                    firstdata = 'No data found'\n",
    "                else:\n",
    "                    file1.writelines(firstdata)\n",
    "        return pd.DataFrame(updated_list),No_Matching_Data # Returns dataframe \n",
    "    \n",
    "    \n",
    "    def Handdle_Blank_link(self,blank_data): # same as above method but this method is used for extracting those text which are not extracted by secondary method\n",
    "        updated_list = []\n",
    "        \n",
    "        for item in blank_data:\n",
    "            i = item['URL_ID']\n",
    "            j = item['URL']\n",
    "            response_code = requests.get(j)\n",
    "            soup = bs(response_code.text, 'html.parser')\n",
    "            article_title = soup.find('title').text\n",
    "            alldiv = soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "            if alldiv is not None:\n",
    "                firstdata = alldiv.text\n",
    "                filename = urllib.parse.quote_plus(j)\n",
    "                file_path = 'C:\\\\Users\\\\vaish\\\\Desktop\\\\project\\\\text_files'\n",
    "                space = \" \"                \n",
    "                with open(f\"{file_path}\\{filename}.txt\", 'w+') as file1:\n",
    "                    file1.writelines(article_title)\n",
    "                    file1.writelines(space)\n",
    "                    file1.writelines(firstdata)             \n",
    "                updated_dict = {\n",
    "                    'URL_ID': i,\n",
    "                    'URL': j,\n",
    "                    'article_words': f\"{article_title} - {firstdata}\"\n",
    "                }\n",
    "                updated_list.append(updated_dict)\n",
    "            else:\n",
    "                print(f\"No data available for the link: {j}\")\n",
    "        df = pd.DataFrame(updated_list)\n",
    "        return df #return a dataframe \n",
    "\n",
    "    def merged(self,df1,df2): #\n",
    "        merged_df = pd.merge(df1, df2, on=['URL_ID', 'URL'], how='left') #merged both dataframe and store in this variable\n",
    "        merged_df = merged_df.dropna()\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        merged_df.to_csv('C:\\\\Users\\\\vaish\\\\Desktop\\\\project\\\\final.csv', index=False) # dataframe is saved in csv format\n",
    "        return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7948a7f-174d-46f2-8bca-500ef8e00f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
